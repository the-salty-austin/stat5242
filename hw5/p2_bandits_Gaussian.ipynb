{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ti4oXuDdyumD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIecl7CJy5W1"
   },
   "source": [
    "# Exercise: Thompson sampling for a 5-armed bandit with Gaussian rewards\n",
    "\n",
    "In this exercise, you will derive a Thompson sampling algorithm for the 5-armed bandit problem with normally distributed rewards. We will use bandits with unit-variance reward distributions, so that $$\n",
    "R_t\\mid A_t=k \\thicksim N(\\theta_k, 1).\n",
    "$$\n",
    "In other words, if we take action $k$, where $k\\in\\{1,2,3,4,5\\}$ at time $t$, then we will receive a reward drawn from a normal distribution with mean $\\theta_k$ and variance 1.\n",
    "\n",
    "Below, we will use the prior distribution $\\mu_k\\thicksim N(0, 1)$.\n",
    "\n",
    "The first few questions will be mathematical derivations for parts of the sampling routine. Then, we will move on to simulating the bandit to see if it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7LMdrXZNRXM"
   },
   "source": [
    "## Deriving Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHQH7Ohs1IjC"
   },
   "source": [
    "Question 1: Normal-normal posterior with known variances\n",
    "\n",
    "Let $X\\thicksim N(a, \\sigma^2)$ be a normally distributed random variable with mean $a$ and variance $\\sigma^2$. Let $Y$ be normally distributed with mean $X$ and variance $\\tau^2$, i.e., $Y\\mid X=x\\thicksim N(x, \\tau^2)$.\n",
    "\n",
    "If we observe $Y=y$, what is the posterior distribution of $X$ given $Y=y$? Hint: use Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcklkzzI1n0X"
   },
   "source": [
    "> Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_frUA5Yc346Q"
   },
   "source": [
    "Question 2: Updating the posterior in Thompson sampling\n",
    "\n",
    "In Thompson sampling, after taking action $A_t=a_t$, we must update the posterior distribution for the bandit's $a_t$th arm. In this case, that means updating our posterior distribution for $\\mu_k$.\n",
    "\n",
    " 1. Say that at iteration $t-1$, our posterior distribution for $\\mu_k$ is $N(m_k(t-1),s^2_k(t-1))$. What is the posterior distribution of $\\mu_k$ at step $t$, after taking action $A_t=k$ and observing $R_t=r_t$? You should phrase your answer in terms of $m_k(t-1)$ and $s^2_k(t-1)$.\n",
    " 2. What is the posterior distribution of $\\mu_k$ after taking action $A_t=a_t$, where $a_t\\neq k$?\n",
    "\n",
    "Hint: use your answer from Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsOJSwWSK_mT"
   },
   "source": [
    "> Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLtRJHXZ1hSx"
   },
   "source": [
    "Question 3: Selecting the play\n",
    "\n",
    "In Thompson sampling, we select our ``play'' or action $A_t$ according to our posterior distribution: select $A_t=k$ with probability equal to our current (posterior) probability that $k$ is the best action.\n",
    "\n",
    "Based on the Bernoulli example from lecture, derive a method for picking $A_t$, and show that it chooses $A_t=k$ with probability equal to $$P(\\mu_k > \\mu_i\\text{ for all }i\\neq k \\mid R_1,\\dots,R_{t-1},A_1,\\dots,A_{t-1}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN064tji1lgQ"
   },
   "source": [
    "> Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5kw3dBWNTfa"
   },
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l5_mSZtOM_3"
   },
   "source": [
    "## Reproducible random number generation in NumPy\n",
    "\n",
    "First, a brief refresher on sampling random variables in NumPy. Of course, you have all done this before, but here is a tutorial in the best practice for reproducible random number generation using NumPy's latest tools. Here, \"reproducible\" means that the simulation will generate the same results across different runs, which is an important tool to have in your data science toolbox when you want to share results that others can replicate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovQiA65Xy3e2",
    "outputId": "43a8c6e8-6b7e-410f-98c6-73846a447f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample: 10.251460442186787\n",
      "Five samples: [ 9.73579027 11.2808453  10.20980023  8.92866125 10.72319011]\n",
      "Samples with 3 means: [1.30400005 5.94708096 9.29626476]\n"
     ]
    }
   ],
   "source": [
    "# here, we instantiate a random generator, which can be used\n",
    "# to sample random variables\n",
    "# this is a Generator object, see: https://numpy.org/doc/stable/reference/random/generator.html\n",
    "# Notice that we set the seed here. This means that\n",
    "# repeated runs of this notebook will have the same\n",
    "# behavior.\n",
    "rg = np.random.default_rng(0)\n",
    "\n",
    "# to draw samples from a normal random variable with mean mu\n",
    "# and standard deviation sigma, we can do it like so:\n",
    "mu = 10\n",
    "sigma = 2\n",
    "sample = rg.normal(loc=mu, scale=sigma)\n",
    "print(\"One sample:\", sample)\n",
    "\n",
    "# to draw multiple samples, we can use\n",
    "samples = rg.normal(loc=mu, scale=sigma, size=5)\n",
    "print(\"Five samples:\", samples)\n",
    "\n",
    "# if we want to sample from multiple means at once\n",
    "# using a shared standard deviation, we can use:\n",
    "mu_vec = [0, 5, 10]\n",
    "sigma = 1\n",
    "mu_samples = rg.normal(loc=mu_vec, scale=sigma)\n",
    "print(\"Samples with 3 means:\", mu_samples)\n",
    "# notice that `mu_samples.shape == (3,)`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew8Eoff0Pr2-"
   },
   "source": [
    "The most important thing to notice above is that the `scale` parameter is the standard deviation, not the variance!\n",
    "\n",
    "Below, please use `rg` to draw your random numbers. The documentation for the `normal` function is here: https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svs0jXRQRe9J"
   },
   "source": [
    "Question 4: coding up a bandit simulation\n",
    "\n",
    "The following code stencil will help you to write a function which simulates the Thompson sampling algorithm derived in Questions 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i30rhkBmPUOf"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_iter = 10000\n",
    "\n",
    "# define the true parameters of the bandit\n",
    "# this is the truth that we want to learn below!\n",
    "mu = rg.normal(loc=0, scale=1, size=5)\n",
    "\n",
    "# setup\n",
    "# These arrays will store the parameters of the current\n",
    "# posterior distribution for mu. Initialize them according\n",
    "# to the prior above.\n",
    "m_t = ... # Your code here\n",
    "s2_t = ... # Your code here\n",
    "\n",
    "# bandit loop\n",
    "rewards = []\n",
    "regrets = []\n",
    "for t in range(n_iter):\n",
    "    # 4.a\n",
    "    # -- pick an action\n",
    "    # Take as many lines as you need to implement the strategy\n",
    "    # derived in question 3. Your action should be a number\n",
    "    # from 0,1,2,3,4 (zero-indexed like Python).\n",
    "    # Your code here\n",
    "    a_t = ...\n",
    "\n",
    "    # 4.b\n",
    "    # -- get a reward\n",
    "    # Sample r_t from the bandit arm corresponding to the action a_t\n",
    "    # Your code here\n",
    "    r_t = ...\n",
    "\n",
    "    # 4.c\n",
    "    # -- update posterior\n",
    "    # Update the posterior parameters for the bandit arm corresponding\n",
    "    # to the action a_t\n",
    "    # Your code here\n",
    "\n",
    "    # 4.d\n",
    "    # -- compute regret\n",
    "    # Compute the regret from the previous iteration, and assign it\n",
    "    # to the variable `regret`\n",
    "    # Your code here\n",
    "    regret = ...\n",
    "\n",
    "    # -- track performance\n",
    "    # Let's keep track of the reward and regret at each iteration\n",
    "    # so that we can plot them below.\n",
    "    rewards.append(r_t)\n",
    "    regrets.append(regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1mQ7CuwjxUf"
   },
   "source": [
    "Question 5: Plotting the results\n",
    "\n",
    "(5.a): Plot the average reward $\\frac{1}{t}\\sum_{i=1}^t r_t$ as a function of $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "wivT5XwfgNzX",
    "outputId": "ebc00e5a-5da9-4a48-d0a2-124e879e1266"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.a: your code here\n",
    "# plot the average reward\n",
    "...\n",
    "\n",
    "# here, we will add the maximum expected reward to the plot\n",
    "# for you to compare against\n",
    "plt.axhline(mu.max(), color=\"k\", label=\"maximum expected reward $\\\\max_k\\\\mu_k$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t39SGtA_kY43"
   },
   "source": [
    "(5.b) Plot the regret using a logarithmic horizontal axis\n",
    "\n",
    "In the lecture, it was mentioned that Thompson sampling asymptotically achieves logarithmic total regret. So, if we plot the total regret using a logarithmic horizontal axis, we should expect to see something that *eventually* looks linear (it won't necessarily at the beginning).\n",
    "\n",
    "Recall, the total regret is the running sum of the regrets at each iteration.\n",
    "\n",
    "Hint: the function `plt.semilogx()` could help! https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF1-UI1xlwmA"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ne_KoKSOCQT5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7gnI7CSH2DA"
   },
   "source": [
    "# Second Part: Gradient bandit\n",
    "\n",
    "In this exercise, you would implement the gradient bandit algorithm for multi-armed bandit problem. We would use the same problem setup as in the thompson sampling problem.\n",
    "\n",
    "Instead of learning the value function, the gradient bandit method directly learns the __policy__ $\\pi$, which is a distribution over all possible actions. In the multi-armed bandit case, the policy is defined through the preference function $H$ as follows:\n",
    "\n",
    "$$\\pi_H(a) \\triangleq Pr(Action = a|H) = \\frac{e^{H(a)}}{\\sum_{b=1}^k e^{H(b)}} \\qquad  (1)$$\n",
    "\n",
    "One key observation is that the preference function $H(a)$ is exactly the _logits_ we've seen in previous exercise. In this case, the higher $H(a)$ is relatively to other $H(a')$, the action $a$ is more __preferred__ than other actions $a'$, hence the name __preference__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxOwdrkjy52Q"
   },
   "source": [
    "## Step 1. The logsumexp trick\n",
    "\n",
    "Directly computing the probability using eq (1) can result in under- or overflow when exponentiating large values.\n",
    "The `logsumexp` trick is a numerically stable way to compute the probability given in eq (1):\n",
    "\n",
    "$$\\pi_H(a)= \\frac{e^{H(a)}}{\\sum_{b=1}^k e^{H(b)}} = \\frac{e^{H(a) - c}}{\\sum_{b=1}^k e^{H(b)-c}}$$\n",
    "where $c = \\max_{b=1}^k e^{H(b)}$.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Implement the function below that computes the probability (i.e. policy) from the logits (i.e. preference) using the `logsumexp` trick.\n",
    "\n",
    "You may use the `scipy.sepcial.logsumexp` function imported below (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNvP8mIoMVbi"
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0BFtOll1qG3"
   },
   "outputs": [],
   "source": [
    "def get_probs(logits):\n",
    "    ### you code here###\n",
    "    # expected: 1-3 lines of codes\n",
    "    # logits is a one-dimensional array, compute the corresponding probability using the logsumexp trick\n",
    "\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsffFP592Jph"
   },
   "source": [
    "## Step 2. Deriving the gradient bandit\n",
    "\n",
    "We would first describe the general policy gradient method, and then discuss its application to the multi-armed bandit problem.\n",
    "\n",
    "The goal of the policy gradient method is to optimize the policy $\\pi_H$ such that the expected reward is maximized, i.e. the optimal $\\pi^*$ is such that\n",
    "\n",
    "$$\\pi^* = \\arg\\max_\\pi \\mathbb{E}_{\\pi(a)} [R(a)]  \\qquad (2)$$\n",
    "\n",
    "### Step 2.1 The REINFORCE trick: a general policy gradient method\n",
    "Suppose the policy $\\pi$ is parameterized by $\\theta$. The optimization problem (2) is equivalent to the following\n",
    "\n",
    "$$\\theta^* = \\arg \\max_\\theta  \\mathbb{E}_{\\pi_\\theta(a)} [R(a)] \\qquad (3)$$\n",
    "\n",
    "To solve this optimization problem, we could use gradient ascent algorithm. Let's first compute the gradient of the expected reward:\n",
    "\n",
    "$$\\nabla_\\theta \\mathbb{E}_{\\pi_\\theta (a)} [R(a)] = \\nabla_\\theta \\int \\pi_\\theta (a) R(a)da $$\n",
    "\n",
    "To compute this gradient exactly, we would need to integrate over all possible actions (the action space could be inifitely-dimensional, or finite-dimension as in the multi-armed bandit problem in which case the integration becomes a summation, but the dimensions could stil be high). We would like to avoid this integration. The REINFORCE trick shows that\n",
    "$$\\nabla_\\theta \\mathbb{E}_{\\pi_\\theta (a)} [R(a)] = \\mathbb{E}_{\\pi_\\theta (a)} [R(a) \\nabla_\\theta \\log \\pi_\\theta(a)] \\qquad (5)$$\n",
    "\n",
    "The implication is that we could obtain a stochastic estimate, and then apply stochastic gradient ascent. That is, randomly sample an action $a$ from $\\pi_\\theta$, and then obtain a stochastic gradient $R(a) \\nabla_\\theta \\log \\pi_\\theta(a)$, which is an unbiased estimate of the true exact gradient $\\nabla_\\theta \\mathbb{E}_{\\pi_\\theta (a)} [R(a)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyrFWyDH8Y7H"
   },
   "source": [
    "\n",
    "### Question 2: Prove the REINFORCE trick eq (5).\n",
    "\n",
    "Hint: Use $\\frac{d}{dx} \\log f(x) = \\frac{1}{f(x)} \\frac{d f(x)}{dx}$.\n",
    "\n",
    "### YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67k5lhAy8WS3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVIkyX0x8-ud"
   },
   "source": [
    "### Introducing the baseline\n",
    "\n",
    "Despite the REINFORCEMENT trick is computationally favorable, it also comes with high variance. One way to reduce the variance of the stochastic gradient estimate is to introduce a baseline $B$ which does not depend on the action: instead of using $R(a) \\nabla_\\theta \\log \\pi_\\theta(a)$ as the stochastic gradient estimate, we use $(R(a) - B) \\nabla_\\theta \\log \\pi_\\theta(a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkqTkrh_-Ikc"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "To justify the use of baseline, we need to make sure the new gradient estimate is also unbiased, i.e. show the following\n",
    "\n",
    "$$  \\mathbb{E}_{\\pi_\\theta (a)} [R(a) \\nabla_\\theta \\log \\pi_\\theta(a)]  = \\mathbb{E}_{\\pi_\\theta (a)} [(R(a)  - B ) \\nabla_\\theta \\log \\pi_\\theta(a)]  \\qquad (6) $$\n",
    "\n",
    "Hint: (1) Use $\\frac{d}{dx} \\log f(x) = \\frac{1}{f(x)} \\frac{d f(x)}{dx}$, (2) exchange gradient with the integral, and (3) remember any probability distribution $\\pi_\\theta $ integrates to 1.\n",
    "\n",
    "### YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz4MtAMe-GC-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FPBX-Hk_Ivs"
   },
   "source": [
    "### Step 2.2 Applying to the multi-armed bandit problem.\n",
    "\n",
    "Let's get back to the multi-armed bandit setting. In this case, the policy $\\pi$ is parameterized by the preference vector $H$, i.e. $\\pi = \\pi_H$. The task is this step is to compute the gradient.\n",
    "\n",
    "In principle, we could use any autodiff method we've learned before to automatically compute $\\nabla_H \\log \\pi_H$. However, we would work out the analytical expression for multi-armed bandit this time.\n",
    "\n",
    "### Question 4\n",
    "Show that\n",
    "\n",
    "$$\\frac{\\partial \\log \\pi_H (A)}{\\partial H(a)} = \\mathbb{1}\\{A=a\\} - \\pi_H(a)\\qquad (7)$$\n",
    "where we recall that $\\pi_H$ is computed using $H$ in eq (1).\n",
    "\n",
    "### YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNGmx3M2Blv2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcmmJTLVDTAD"
   },
   "source": [
    "### Step 2.3 Instantiating the gradient ascent rule\n",
    "\n",
    "Having derived the gradient estimate, we can now write down the update rule for the preference parameter using the gradient ascent method:\n",
    "\n",
    "$$H_{t+1}(a) \\leftarrow H_t(A_t) + \\alpha (R_t - B_t) (\\mathbb{1} \\{A_t = a\\} -\\pi_t(a)), \\forall a  \\qquad (8)$$\n",
    "\n",
    "where $A_t, R_t, B_t$ are the action, reward and baseline at iteration $t$, and $\\alpha$ is the learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4MBivgsEbo6"
   },
   "source": [
    "### Step 3. Implement the gradiant bandit.\n",
    "\n",
    "### Question 5\n",
    "\n",
    "In the following code, implement the gradient bandit problem. Use the average reward $\\bar R_t$ as baseline. More specifically, when $t=0$, set $\\bar R_t = 0$. When $t \\geq 1$, set $\\bar R_t =\\frac{1}{t} \\sum_{i=0}^{t-1} R_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZMVDOrzG6tm"
   },
   "outputs": [],
   "source": [
    "def gradient_bandit(n_iter, H_init, alpha, use_baseline=False):\n",
    "    H_t = H_init\n",
    "    K = H_init.shape[0]\n",
    "\n",
    "    rewards = []\n",
    "    regrets = []\n",
    "    actions = []\n",
    "    Hs = []\n",
    "\n",
    "    baseline = 0\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        # 5.a\n",
    "        # compute the policy and then sample an action from the policy\n",
    "        ### YOUR CODE HERE####\n",
    "        # expected two lines of codes\n",
    "        pi_t =\n",
    "        a_t =\n",
    "        ######################\n",
    "        actions.append(a_t)\n",
    "\n",
    "        if use_baseline and t >= 1:\n",
    "          # 5.b\n",
    "          # update the baseline\n",
    "          ### YOUR CODE HERE ###\n",
    "          # expected 1 line of code\n",
    "          baseline =\n",
    "          ######################\n",
    "\n",
    "        # 5.c\n",
    "        # get a reward\n",
    "        # Sample r_t from the bandit arm corresponding to the action a_t\n",
    "        ### YOUR CODE HERE ###\n",
    "        r_t =\n",
    "        ######################\n",
    "\n",
    "        # 5.d\n",
    "        # update policy\n",
    "        ### YOUR CODE HERE ###\n",
    "        # expected 3-6 lines of codes\n",
    "\n",
    "        H_t =\n",
    "        ######################\n",
    "        Hs.append(H_t)\n",
    "\n",
    "        # 5.e\n",
    "        # -- compute regret\n",
    "        # Compute the regret from the previous iteration, and assign it\n",
    "        # to the variable `regret`\n",
    "        ### Your code here ###\n",
    "        regret =\n",
    "        ######################\n",
    "\n",
    "        # -- track performance\n",
    "        # Let's keep track of the reward received at each iteration\n",
    "        # so that we can plot it below.\n",
    "        rewards.append(r_t)\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return H_t, pi_t, rewards, regrets, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tri4O80iWLGn"
   },
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "alpha  = 0.1\n",
    "H_nobaseline, pi_nobaseline, rewards_nobaseline, regrets_nobaseline, actions_nobaseline = gradient_bandit(n_iter, np.zeros(5), alpha, use_baseline=False)\n",
    "H_baseline, pi_baseline, rewards_baseline, regrets_baseline, actions_baseline = gradient_bandit(n_iter, np.zeros(5), alpha, use_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snejo7NgIUP6"
   },
   "source": [
    "You can view the final policies as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiWih_3qIEjR"
   },
   "outputs": [],
   "source": [
    "plt.plot(pi_nobaseline,'-.', label='without baseline')\n",
    "plt.plot(pi_baseline,'--', label='with baseline')\n",
    "plt.plot(mu, '-o', label='mu')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgryjg6PHW52"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Plotting the results as you did for thompson sampling problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEe8iVUFH4I8"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# 6.a: plot the average reward\n",
    "\n",
    "# 6.b plot the cumulative regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyKj65u_HjJQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxZesbMnwqaC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
