{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5vtG0bbJt9u"
   },
   "source": [
    "# **GR5242 HW04 Problem 1: Shakespeare with LSTM networks**\n",
    "\n",
    "**Instructions**: This problem is an individual assignment -- you are to complete this problem on your own, without conferring with your classmates.  You should submit a completed and published notebook to Courseworks; no other files will be accepted.\n",
    "\n",
    "## Description:\n",
    "\n",
    "This homework exercise has 3 primary goals:\n",
    " * Introduce some basic concepts from natural language processing\n",
    " * Get some practice training recurrent neural networks, specifically on text data\n",
    " * Be able to generate fake text data from your favorite author!   \n",
    "\n",
    "By the end of this exercise, you will have a basic, but decent, computer program which can simulate the writing patterns of any author of your choice.\n",
    "\n",
    "Here is an outline of the rest of the exercise.\n",
    " 1. Data loading\n",
    "     - We will start by downloading a text from Project Gutenberg that we will try to model\n",
    "     - Data preprocessing and numerical encoding\n",
    "     - Making training `Dataset` and `DataLoader` objects\n",
    " 3. Learn to generate text with a neural network\n",
    "     - Defining the recurrent network\n",
    "     - Training\n",
    "     - Predicting and sampling text from the model\n",
    "\n",
    "     There are 12 questions (70 points) in total, which include coding and written questions. You can only modify the codes and text within \\### YOUR CODE HERE ### and/or \\### YOUR ANSWER HERE ###.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6hwabXFdkMlH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_-QkOsDQrgu"
   },
   "source": [
    "## Character-level language modeling\n",
    "\n",
    "Our goal here is to build a model of language letter-by-letter. Since we may also allow numbers, spaces, and punctuation, it's better to say character-by-character. We will start by fixing an \"alphabet\": the set of allowed characters.\n",
    "\n",
    "In math notation, let's call the alphabet $A$. In code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K8RdHuHPRCPJ"
   },
   "outputs": [],
   "source": [
    "alphabet = \" \"\"'abcdefghijklmnopqrstuvwxyz1234567890.,!?:;ABCDEFGHIJKLMNOPQRSTUVWXYZ\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sitlEu4BJsBm"
   },
   "source": [
    "# Section 1: Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oryq1WYLNojq"
   },
   "source": [
    "We will start by downloading training data from Project Gutenberg: https://www.gutenberg.org/. Project Gutenberg is a free repository of public domain books. Find any book you like, and download it in Plain Text UTF-8 format.\n",
    "\n",
    "For example, we will use Shakespeare's complete works: https://www.gutenberg.org/ebooks/100. There is a link on that page to the Plain Text format data.  Download the pg100.txt file, and then upload it from your computer to colab (click at left on the File icon, then click the upload icon).  \n",
    "\n",
    "*Important*: whichever work you choose, make sure you have enough data! The size of your plain text file should be at least 2MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeGF0qSbJqqB",
    "outputId": "b4bcb981-dd3d-41d4-beb6-ac5058cc0a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Moby Dick; Or, The Whale\n",
      "\n",
      "Author: Herman Melville\n",
      "\n",
      "Release date: July 1, 2001 [eBook #2701]\n",
      "                Most recently updated: August 18, 2021\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Daniel Lazarus, Jonesey, and David Widger\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MOBY-DICK;\n",
      "\n",
      "or, THE WHALE.\n",
      "\n",
      "By Herman Melville\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "EXTRACTS (Supplied by a Sub-Sub-Librarian).\n",
      "\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "CHAPTER 2. \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the book's plain text file on Project Gutenberg\n",
    "url = \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\"  # Moby Dick; Or, The Whale\n",
    "\n",
    "# Fetch and read the book\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Display the first 1000 characters to verify\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNYDQKRSXBk"
   },
   "source": [
    "Let's load the text and see what it says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RKK_A1NShRW",
    "outputId": "032688ce-4c2f-403e-ea2e-c3e41054b57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is 1260542 characters long.\n",
      "\n",
      "A sample from the middle:\n",
      "\n",
      "whole boat in its complicated coils,\n",
      "twisting and writhing around it in almost every direction. All\n"
     ]
    }
   ],
   "source": [
    "print(\"text is\", len(text), \"characters long.\")\n",
    "print()\n",
    "print(\"A sample from the middle:\")\n",
    "print()\n",
    "print(text[len(text) // 2 : len(text) // 2 + 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7viKRe94P_Cx"
   },
   "source": [
    "### Data standardization\n",
    "\n",
    "Now, we will clean the data: converting the data to lowercase, removing extra spaces and linebreaks, and get rid of characters which are not in our alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5m3ZqU7RS2xA"
   },
   "outputs": [],
   "source": [
    "# remove extra characters by replacing them with spaces\n",
    "text = re.sub(rf\"[^{alphabet}]\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tgMpHS2O67G"
   },
   "source": [
    "Let's see how it looks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TiJEL-O-O6wL",
    "outputId": "5e40729d-31c0-44c4-fe48-283735111f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n"
     ]
    }
   ],
   "source": [
    "a = 110042\n",
    "b = a+131\n",
    "x_prompt = text[a:b]\n",
    "print(x_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JFrb5Jr92F8"
   },
   "source": [
    "### Numerical encoding\n",
    "\n",
    "Unfortunately, neural networks don't understand text. So, we need to convert our characters to numerical values. Here are some helper functions for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1LjvsaNy91to"
   },
   "outputs": [],
   "source": [
    "# let's build a dictionary mapping characters to integers\n",
    "char2int = {c: i for i, c in enumerate(alphabet)}\n",
    "alphabet_array = np.array([c for c in alphabet])\n",
    "\n",
    "# this function will turn a string into a numpy array of integers\n",
    "def int_encode(string):\n",
    "    if any(c not in char2int for c in string):\n",
    "        raise ValueError(\n",
    "            \"Found a character which was not in the alphabet in the input \"\n",
    "            f\"to int_encode. Valid alphabet characters: {alphabet}\"\n",
    "        )\n",
    "    return np.array([char2int[c] for c in string])\n",
    "\n",
    "# this function will decode a numpy array of integers back to a string\n",
    "def int_decode(int_array):\n",
    "    return ''.join(alphabet_array[int_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsSPJalSWFUm"
   },
   "source": [
    "(Question 1a: 4 points) Test out `int_encode` by passing `test_string` in and printing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vU8Umqsw-4CN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62 63 44 63 32 29 31 29 42  0 44  5 23  2 15  4  6  5 70 56  2  4  9 10\n",
      " 15  6 39  0 55  6  2 19 15 10 15  8 40  0 46 16 22 19 20  6 41]\n"
     ]
    }
   ],
   "source": [
    "# Let's test these out!\n",
    "### YOUR CODE HERE ###\n",
    "test_string = \"STAT5242: Advanced\\nMachine, Learning! Course?\"\n",
    "e = int_encode(test_string)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGFYXZPcWL8Z"
   },
   "source": [
    "(Question 1b: 4 points) Decode the result from the last cell using `int_decode` to make sure it is the same as `test_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "392dWPKn0J85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT5242: Advanced\n",
      "Machine, Learning! Course?\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "d = int_decode(e)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWxJykne_VKY"
   },
   "source": [
    "Is the decoding the same as `test_string`? It should -- you have a bug above if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIqRJLtyAC--"
   },
   "source": [
    "### Make a training dataset\n",
    "\n",
    "First, we make a numerical encoded version of the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JOr_T3ddALvQ"
   },
   "outputs": [],
   "source": [
    "enctext = int_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1FLc9MABXKu"
   },
   "source": [
    "Use `torch.tensor` to make it into a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORDJL4_0BWTK",
    "outputId": "6dea873a-ea3b-44e1-95dd-a58d653df8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 63,  9,  ..., 70,  0, 70], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "enctext = torch.tensor(enctext)\n",
    "print(enctext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1260542])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enctext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk3z0bACJyQc"
   },
   "source": [
    "# Section 2: Training a NN\n",
    "\n",
    "Our model will work as follows:\n",
    " - One-hot encoded input gets passed into a linear embedding layer. These two operations are combined with the `Embedding` layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    " - LSTM cell\n",
    " - Linear decoder layer\n",
    "\n",
    "Torch has two main ways of interfacing with recurrent networks. In the case of LSTMs, those are:\n",
    " - the LSTM layer https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    " - the LSTMCell layer https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "\n",
    "Both models are sequential: the goal is to process a batch of sequences of input features and produce a batch of sequences of output features. The `LSTM` class makes this simple and easy, and the `LSTMCell` class gives more control by allowing you to process the sequences one element at a time. We will use the `LSTM` layer to keep things simple, but keep in mind that some of what we do could be made more efficient with `LSTMCell`.\n",
    "\n",
    "The inputs and outputs to recurrent networks in Torch have shape: `(batch_dimension, sequence_dimension, feature_dimension)`. In this case, our feature dimension is `len(alphabet)`.\n",
    "\n",
    "Something to keep in mind: the output of this network will be stateful! In each batch, the `k`th output along the sequence dimension will be the logits for predicting the `k+1`th input in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F80hsi-FH4WA"
   },
   "outputs": [],
   "source": [
    "# We will use this constant below\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_ZUt-1--LP32"
   },
   "outputs": [],
   "source": [
    "# Defining some parameters about data batching, explained in the next section\n",
    "# Note: after you get the entire assignment working, you can make these\n",
    "# bigger and train for longer, to get better performance\n",
    "SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2ORoPzNJkKP"
   },
   "source": [
    "### Making the dataset of (input, target) pairs\n",
    "\n",
    "To train the model, we need to make a `torch.utils.data.Dataset` containing input and target sequences. Our input sequences will be sequences of length `SEQUENCE_LENGTH` containing int-encoded characters from the input. Our target sequences will be the \"next characters\" corresponding to the input sequence: so, if the input sequence is the 10th, 11th, ... characters, then the target sequence is the 11th, 12th, ... characters.\n",
    "\n",
    "We will walk through using `torch.utils.data.Dataset` methods to create these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PlSDJOjL_Fb"
   },
   "source": [
    "(Question 2c: 8 points) Write a `batch` function for a `torch` tensor, which we defined above, to make disjoint consecutive sequences of consecutive characters of length `SEQUENCE_LENGTH`. `torch.split()` and `torch.vstack()` may be useful. Remember to be careful of the edge case that arises when `len(enctext) % SEQUENCE_LENGTH != 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dX6fJc84W7Lo"
   },
   "outputs": [],
   "source": [
    "def batch(enctext: torch.Tensor, seqlen: int):\n",
    "    # YOUR CODE HERE\n",
    "    n = enctext.shape[0]\n",
    "    remainder = n % seqlen\n",
    "    batches = torch.split(enctext[:n - remainder], seqlen)\n",
    "    if remainder:\n",
    "        last_batch = torch.hstack([\n",
    "                enctext[n - remainder:],\n",
    "                torch.tensor([0] * (seqlen - remainder))\n",
    "            ]) # padding\n",
    "        return torch.vstack([torch.vstack(batches), last_batch])\n",
    "    return torch.vstack(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n24Q5ySjU2ed"
   },
   "source": [
    "(Question 2d: 8 points) Now, use batch to create target sequences from the following version of the dataset which has been offset by 1 element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BkjMZfS3XAae"
   },
   "outputs": [],
   "source": [
    "batches = batch(enctext, SEQUENCE_LENGTH)\n",
    "input_seqs  = batches[:-1]\n",
    "target_seqs = batches[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9847, 128]), torch.Size([9847, 128]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geD7pQ7TVH1Z"
   },
   "source": [
    "(Question 2e: 6 points) Now, use the `torch` builtin class `torch.utils.data.TensorDataset` to create a dataset of (input, target) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5rrw7OLXGjS",
    "outputId": "4662f219-ad1a-4e27-8a1b-a97d0b61d773"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x1db4ce6d310>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = TensorDataset(input_seqs, target_seqs)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX1_4b0oVtdv"
   },
   "source": [
    "(Question 2f: 4 points) Finally, define a `torch.utils.data.DataLoader` object to generate batches of pairs of length `BATCH_SIZE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pE547bYpXJge",
    "outputId": "493d534a-ed6d-4067-cc77-f47b942ce0e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1db4ce97050>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(pairs, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcgjwlrDMOdz"
   },
   "source": [
    "You may uncomment the below cell if you would like to understand the structure of the `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d5q07j73MOdz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 128]) torch.Size([64, 128])\n",
      "1 torch.Size([64, 128]) torch.Size([64, 128])\n",
      "2 torch.Size([64, 128]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(i, x.shape, y.shape)\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2syDhfkWdXt"
   },
   "source": [
    "(Question 2a: 10 points) Model definition: make a Sequential model with an Embedding layer with input dimension `len(alphabet)` and output dimension `HIDDEN_DIM`, followed by an LSTM layer with `HIDDEN_DIM` features, followed by a Linear layer with `len(alphabet)` features. A helper class is provided to extract tensors from the output of the LSTM layer to prepare as input to the input of the final linear layer. Use of this class in the Sequential container would look something like `('extract', extract_tensor(return_sequences=return_sequences))`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "REnQC0MXiyAL"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "return_sequences = True\n",
    "\n",
    "# LSTM() returns tuple of (tensor, (recurrent state))\n",
    "class extract_tensor(nn.Module):\n",
    "    def __init__(self, return_sequences=False):\n",
    "        super(extract_tensor, self).__init__()\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        tensor, _ = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        if not self.return_sequences:\n",
    "            tensor = tensor[:, -1, :]\n",
    "        return tensor\n",
    "\n",
    "# input: (BATCH_SIZE, len(alphabet)) = (64, 71)\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('embedding', nn.Embedding(len(alphabet), HIDDEN_DIM)), # -> (BATCH_SIZE, SEQUENCE_LENGTH, embedding_dim) = (64, 128, 128)\n",
    "    ('lstm', nn.LSTM(HIDDEN_DIM, HIDDEN_DIM)), # -> (BATCH_SIZE, SEQUENCE_LENGTH, HIDDEN_DIM) = (64, 128, 128)\n",
    "    ('extract', extract_tensor(return_sequences=return_sequences)), # -> (BATCH_SIZE, SEQUENCE_LENGTH, HIDDEN_DIM) = (64, 128, 128)\n",
    "    ('linear', nn.Linear(HIDDEN_DIM, len(alphabet))) # -> (BATCH_SIZE, SEQUENCE_LENGTH, len(alphabet)) = (64, 128, 71)\n",
    "]))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (embedding): Embedding(71, 128)\n",
      "  (lstm): LSTM(128, 128)\n",
      "  (extract): extract_tensor()\n",
      "  (linear): Linear(in_features=128, out_features=71, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 128\n",
      "Layer: embedding.weight | Shape: torch.Size([71, 128])\n",
      "Layer: lstm.weight_ih_l0 | Shape: torch.Size([512, 128])\n",
      "Layer: lstm.weight_hh_l0 | Shape: torch.Size([512, 128])\n",
      "Layer: lstm.bias_ih_l0 | Shape: torch.Size([512])\n",
      "Layer: lstm.bias_hh_l0 | Shape: torch.Size([512])\n",
      "Layer: linear.weight | Shape: torch.Size([71, 128])\n",
      "Layer: linear.bias | Shape: torch.Size([71])\n"
     ]
    }
   ],
   "source": [
    "print(len(alphabet), HIDDEN_DIM)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding.weight | Shape: torch.Size([71, 128]):\n",
    "\n",
    "    71 unique characters (alphabet) and embed each into a 128-dimensional space.\n",
    "\n",
    "Layer: lstm.weight_ih_l0 | Shape: torch.Size([512, 128]):\n",
    "\n",
    "    input-to-hidden weight matrix for LSTM. [512, 128]: it takes a 128-dimensional input (the output of the embedding layer) and transforms it into a 512-dimensional hidden state. 512 comes from that an LSTM has 4 internal gates (input, forget, cell, output), and each gate has its own weight matrix of size [HIDDEN_DIM, input_size], so 4 * 128 = 512.\n",
    "\n",
    "Layer: lstm.weight_hh_l0 | Shape: torch.Size([512, 128]):\n",
    "\n",
    "    hidden-to-hidden weight matrix within LSTM. It takes the previous 128-dimensional hidden state and transforms it into a new 512-dimensional hidden state (again, due to the 4 gates).\n",
    "\n",
    "Layer: lstm.bias_ih_l0 | Shape: torch.Size([512]):\n",
    "\n",
    "    Bias terms for the input-to-hidden connections.\n",
    "\n",
    "Layer: lstm.bias_hh_l0 | Shape: torch.Size([512]):\n",
    "\n",
    "    Bias terms for the hidden-to-hidden connections.\n",
    "\n",
    "Layer: linear.weight | Shape: torch.Size([71, 128]):\n",
    "\n",
    "    This is your final linear layer. It takes the 128-dimensional output of the LSTM (after your extract_tensor) and maps it to 71 output classes (one for each character in your alphabet).\n",
    "\n",
    "Layer: linear.bias | Shape: torch.Size([71]):\n",
    "\n",
    "    Bias terms for the linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzp4Q7vFJXVj"
   },
   "source": [
    "(Question 2b: 8 points) If we want to use the output of the model as logits for predicting a character (which we can think of as a class), what loss should we use? Name this `criterion`. Additionally, define an optimizer to use in training. As per usual, we will recommend the use of `optim.Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "u1XFATDzieB5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psw5Vbpk3J3Q"
   },
   "source": [
    "(Question 2g: 2 points) Train the model!\n",
    "\n",
    "\n",
    "Write a training loop in PyTorch for a model that processes batched input data. Use `NUM_EPOCHS = 40` as the number of training epochs, and ensure that:\n",
    "1. Each epoch consists of iterating over batches from a `train_loader`.\n",
    "2. For each batch, the model's gradients are zeroed, a forward pass is made, and a loss is calculated using a provided criterion.\n",
    "3. After each batch's loss is calculated, perform backpropagation and optimizer steps.\n",
    "4. Track and print the average loss at the end of each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wrOmFS_eV21G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/15]: 100%|██████████| 154/154 [00:01<00:00, 112.95it/s, loss=3.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/15, Loss: 36568.758, Accuracy: 14.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/15]: 100%|██████████| 154/154 [00:01<00:00, 138.78it/s, loss=3.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/15, Loss: 26029.649, Accuracy: 17.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/15]: 100%|██████████| 154/154 [00:01<00:00, 139.25it/s, loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/15, Loss: 27057.892, Accuracy: 16.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/15]: 100%|██████████| 154/154 [00:01<00:00, 140.86it/s, loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 4/15, Loss: 25803.391, Accuracy: 18.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/15]: 100%|██████████| 154/154 [00:01<00:00, 144.01it/s, loss=3.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 5/15, Loss: 25846.772, Accuracy: 17.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/15]: 100%|██████████| 154/154 [00:01<00:00, 139.29it/s, loss=3.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 6/15, Loss: 25623.713, Accuracy: 18.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/15]: 100%|██████████| 154/154 [00:01<00:00, 140.72it/s, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 7/15, Loss: 25593.065, Accuracy: 18.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/15]: 100%|██████████| 154/154 [00:01<00:00, 137.04it/s, loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 8/15, Loss: 25624.168, Accuracy: 18.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/15]: 100%|██████████| 154/154 [00:01<00:00, 143.20it/s, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 9/15, Loss: 25721.262, Accuracy: 17.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/15]: 100%|██████████| 154/154 [00:01<00:00, 145.20it/s, loss=3.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 10/15, Loss: 25735.352, Accuracy: 17.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/15]: 100%|██████████| 154/154 [00:01<00:00, 138.67it/s, loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 11/15, Loss: 25621.486, Accuracy: 18.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/15]: 100%|██████████| 154/154 [00:01<00:00, 140.30it/s, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 12/15, Loss: 25679.202, Accuracy: 18.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/15]: 100%|██████████| 154/154 [00:01<00:00, 147.79it/s, loss=3.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 13/15, Loss: 25685.797, Accuracy: 17.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/15]: 100%|██████████| 154/154 [00:01<00:00, 146.85it/s, loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 14/15, Loss: 25729.259, Accuracy: 17.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/15]: 100%|██████████| 154/154 [00:01<00:00, 145.37it/s, loss=11.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 15/15, Loss: 36714.137, Accuracy: 15.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "n_epochs = 15\n",
    "n_train = len(train_loader) # number of batches\n",
    "\n",
    "print(\"start to train\")\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loop = tqdm.tqdm(train_loader, leave=True)\n",
    "    for inputs, labels in loop:\n",
    "        # print(inputs[:1], labels[:1])\n",
    "        inputs = inputs.to(device) # torch.Size([BATCH_SIZE, SEQUENCE_LENGTH]) = (64,128)\n",
    "        labels = labels.to(device) # torch.Size([BATCH_SIZE, SEQUENCE_LENGTH]) = (64,128)\n",
    "        # labels = nn.functional.one_hot(labels, num_classes=len(alphabet))  # (64, 128, 71)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) # -> torch.Size([64, 128, 71])\n",
    "        outputs = outputs.view(-1, len(alphabet)) # -> (64*128, 71) = (8192, 71)\n",
    "        labels = labels.view(-1) # -> (64*128, ) = (8192, )\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # predicted_classes = outputs.argmax(dim=1) # -> (8192, )\n",
    "        _, predicted_classes = torch.max(nn.functional.softmax(outputs, dim=1), 1) # -> (8192, )\n",
    "        batch_accuracy = (predicted_classes == labels).float().mean()\n",
    "        total_accuracy += batch_accuracy.item()\n",
    "        total_loss += loss.item() * len(labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate the average accuracy for the entire epoch\n",
    "    average_accuracy = total_accuracy / n_train\n",
    "    average_loss = total_loss / n_train\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\" Epoch {epoch + 1}/{n_epochs}, Loss: {average_loss:.3f}, Accuracy: {average_accuracy * 100:.2f}%\")\n",
    "    #print(\"len train_loader is \" + str(len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbYcgKmj1zL"
   },
   "source": [
    "Here, make sure the loss goes down as it trains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmysazCNc14W"
   },
   "source": [
    "# Section 3: Did it work? Let's see what the model learned\n",
    "\n",
    "Here, we'll write some functions to see how well the model has learned to predict text and to draw samples from the model.\n",
    "\n",
    "First, we'll give you a function to \"seed\" the model with some input text and then predict the most likely future text. It will be your job to create a variation on this function in the question below, so make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UUaB5I6VXTdi"
   },
   "outputs": [],
   "source": [
    "def predict(seed_string, sample_length=50):\n",
    "    # Convert seed_string to int\n",
    "    current_text_ints = list(int_encode(seed_string))\n",
    "    # print(current_text_ints)\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        # Add an empty batch dimension and convert to tensor\n",
    "        text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "        text_arr = torch.tensor(text_arr).to(device)\n",
    "        # print(text_arr)\n",
    "\n",
    "        # set our model to return only one output instead of the sequence\n",
    "        model.extract.return_sequences = False\n",
    "\n",
    "        # Get the full sequence of predictions, remove the batch dim\n",
    "        logits = model(text_arr)\n",
    "        # print(logits)\n",
    "\n",
    "        # Remove the batch dimension and get the final logits\n",
    "        final_logits = logits[-1]\n",
    "        # print(final_logits)\n",
    "\n",
    "        # Get the prediction using tf.argmax\n",
    "        pred = torch.argmax(final_logits)\n",
    "        # print(pred)\n",
    "        # print('--------------')\n",
    "\n",
    "        # Append this to `current_text_ints`\n",
    "        # current_text_ints.append(pred.numpy())\n",
    "        current_text_ints.append(pred.item())\n",
    "\n",
    "    return int_decode(np.array(current_text_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ht1OAtegjv",
    "outputId": "a48694b3-4280-4b67-ac3d-7e7ae685734a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt (x_prompt) used for prediction:\n",
      "131 But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n",
      "\n",
      "Extracted substring (x_prompt_plus) from 'text' starting at index 110042 up to index 110323 :\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel s \n",
      "face; and this bright face shed a distinct spot of radiance upon the \n",
      "ship s tossed deck, something like that silver plate now inserted into \n",
      "the V\n",
      "\n",
      "Predicted text based on x_prompt with a prediction length of 150 characters:\n",
      "281 280 But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m \n"
     ]
    }
   ],
   "source": [
    "pred_length = 150\n",
    "\n",
    "# Print the initial prompt (x_prompt) used for prediction\n",
    "print(\"Initial prompt (x_prompt) used for prediction:\")\n",
    "print(len(x_prompt), x_prompt)  # Assuming x_prompt is already defined\n",
    "\n",
    "a = 110042\n",
    "bb = a + 131 + pred_length\n",
    "x_prompt_plus = text[a:bb]  # Extracting substring from text\n",
    "\n",
    "# Print the extracted substring from 'text' within the specified range\n",
    "print(\"\\nExtracted substring (x_prompt_plus) from 'text' starting at index\", a, \"up to index\", bb, \":\")\n",
    "print(x_prompt_plus)\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nPredicted text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "\n",
    "pred_result = predict(x_prompt, pred_length)\n",
    "print(len(pred_result), len(pred_result.strip()), pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1gT8Oi_KgocR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellod m m m m  15\n"
     ]
    }
   ],
   "source": [
    "# feel free to try your own seed!\n",
    "p = \"Hello\"\n",
    "res = predict(p, 10)\n",
    "print(res, len(res)) # it's simply \"predicting\" lots of m's and spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5f92OC-gORk"
   },
   "source": [
    "It seems like maybe the model learned something, but the output is a little boring. Let's make it more interesting with *randomness*!\n",
    "\n",
    "Right now, the function always picks the most likely next letter. Instead, let's sample the next letter from the model's predicted probability distribution.\n",
    "\n",
    "(Question 3a: 8 points) Fill in the blanks in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[ 1.1882e+01, -2.1653e+01,  7.3202e+00,  1.0307e+01,  9.2052e+00,\n",
    "          1.2288e+01,  1.0670e+01, -5.2825e-01, -7.5656e-01,  9.4695e+00,\n",
    "          7.0515e+00, -6.8845e+00,  3.6359e+00,  3.6441e+00,  7.9128e+00,\n",
    "          1.1908e+01,  9.1662e+00,  7.3278e+00, -8.8111e+00,  7.7560e+00,\n",
    "          8.8169e+00,  1.0833e+01,  1.0459e+01, -9.0431e+00, -6.9849e-01,\n",
    "         -2.5091e+01,  5.1266e+00, -3.3887e+01, -1.1239e+01, -4.7748e+01,\n",
    "          1.8662e-02, -3.3456e+01, -9.3685e+00, -3.3035e+01, -1.9563e+01,\n",
    "         -1.6129e+01, -1.4965e+01, -1.7602e+00, -6.3772e+00,  1.0108e+01,\n",
    "          5.1536e+00,  4.1522e+00, -3.6856e+01, -1.7400e+01, -4.5224e+01,\n",
    "          3.3552e+00, -2.9813e+00,  2.7603e+00, -4.3596e+00,  3.4254e+00,\n",
    "         -4.3740e+00,  4.8554e+00, -8.1969e+00, -4.2921e+01, -7.3727e+01,\n",
    "         -1.2317e+01, -7.8261e+00,  1.4202e+00, -1.3906e+00, -1.8264e+01,\n",
    "         -2.6762e+00,  4.4032e+00, -4.0476e+01,  4.2402e+00, -1.3938e+01,\n",
    "         -4.9166e+00, -1.8416e+01, -4.0483e+01, -1.8585e+01, -8.0525e+00,\n",
    "          7.4953e+00]])\n",
    "\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "torch.multinomial(probabilities, num_samples=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "m4Oo888KiQyS"
   },
   "outputs": [],
   "source": [
    "def generate(seed_string, sample_length=50):\n",
    "    # Convert seed_string to int\n",
    "    current_text_ints = list(int_encode(seed_string))\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        # Add an empty batch dimension and convert to tensor\n",
    "        text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "        text_arr = torch.tensor(text_arr).to(device)\n",
    "\n",
    "        # set our model to return only one output instead of the sequence\n",
    "        model.extract.return_sequences = False\n",
    "\n",
    "        # Get the full sequence of predictions, remove the batch dim\n",
    "        logits = model(text_arr)\n",
    "\n",
    "        # Remove the batch dimension and get the final logits\n",
    "        final_logits = logits[-1]\n",
    "\n",
    "        # Normalize the final_logits to a probability distribution\n",
    "        probs = F.softmax(final_logits, dim=0)  # YOUR CODE HERE\n",
    "\n",
    "        # Call .numpy so we can use a numpy function\n",
    "        # probs = probs.numpy()\n",
    "\n",
    "        # Sample from the probability distribution using\n",
    "        # the function np.random.choice - doesnt work for dimension >= 32\n",
    "        sample = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Append this to `current_text_ints`\n",
    "        current_text_ints.append(sample)\n",
    "\n",
    "    return int_decode(np.array(current_text_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_text_ints = list(int_encode('heelo'))\n",
    "# text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "# text_arr = torch.tensor(text_arr).to(device)\n",
    "# model.extract.return_sequences = False\n",
    "\n",
    "# # Get the full sequence of predictions, remove the batch dim\n",
    "# logits = model(text_arr)\n",
    "\n",
    "# # Remove the batch dimension and get the final logits\n",
    "# final_logits = logits[-1]\n",
    "\n",
    "# # Normalize the final_logits to a probability distribution\n",
    "# print(final_logits)\n",
    "# probs = F.softmax(final_logits, dim=0)\n",
    "# probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp07ZVlUhtpy"
   },
   "source": [
    "(Question 3b: 6 points) Test this function `generate`. Is its output different from `predict`? How does it differ, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d75C0gswiOU2",
    "outputId": "84ea4204-cb22-42a7-ea41-b504b8b158f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt (x_prompt) used for prediction:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n",
      "\n",
      "Extracted substring (x_prompt_plus) from 'text' starting at index 110042 up to index 110323 :\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel s \n",
      "face; and this bright face shed a distinct spot of radiance upon the \n",
      "ship s tossed deck, something like that silver plate now inserted into \n",
      "the V\n",
      "\n",
      "Predicted text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m \n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel u uhd mne h mnmn\n",
      "   mmeonponp ,be t mt mt ub uu monpon0ubeo mes e ubnnpod m mnpt mn;e me me\n",
      "npod mt eoc ubdubube mn;t e mnpt  m ud esobnnpt ult lt m m\n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel le ubt u,eoumnptudddnpoo ln;dnpoud hubnn; lhddd,uumes mnpt mnpt uudubs ,eahd m uubt esouum lm mt  ldnnpt  bnpod me lt udn ,dduubnsu me mnnn\n",
      ",e t ubu u\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Print the initial prompt (x_prompt) used for prediction\n",
    "print(\"Initial prompt (x_prompt) used for prediction:\")\n",
    "print(x_prompt)  # Assuming x_prompt is already defined\n",
    "\n",
    "a = 110042\n",
    "bb = a + 131 + pred_length\n",
    "x_prompt_plus = text[a:bb]  # Extracting substring from text\n",
    "\n",
    "# Print the extracted substring from 'text' within the specified range\n",
    "print(\"\\nExtracted substring (x_prompt_plus) from 'text' starting at index\", a, \"up to index\", bb, \":\")\n",
    "print(x_prompt_plus)\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nPredicted text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(predict(x_prompt, pred_length))\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n",
    "\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel m ud mt  meo mnpt ub le me e mus meobt ud m mht hdd ud mnnpo   e ,e mt mt emuubnpt m me ubd mnpot  e uub d ,e mt mt  lh m t ld unponpt m mn\n",
      "dnpo le ub\n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel m me le , ls meod mct mnnnn0 mis mb leodn\n",
      " m mt me m ,e ,npohct ubnponp esubnnnpodddd ududut ub m u,dddh,e ,npt mnpt meod me mnpt mm mnpt m me mn; mt \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy3J9i71hnWT"
   },
   "source": [
    "(Question 3c: 2 point) Try running `generate` a few times. Are the results the same or different? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are difference since there is randomness. However, there seems to be characters that show up much more frequently than others. (Or, some characters do not appear at all.)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
