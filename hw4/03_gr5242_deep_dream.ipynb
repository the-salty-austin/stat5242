{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofEjkCI7MPTh"
   },
   "source": [
    "# **GR5242 HW01 Problem 3: Early stopping and basic \"deep dream\"**\n",
    "\n",
    "### Fill in your code below `############# YOUR CODE HERE #############` and answers to reflection questions in the text box with **Your Answer Here**\n",
    "\n",
    "In this exercise, you will explore some basic methods for preventing overfitting (early stopping and dropout) and explore model introspection by a basic version of Alex Mordvintsev's famous [\"deep dream\" experiment][deepdream].\n",
    "\n",
    "## Early stopping and dropout\n",
    "\n",
    "If we train a model which has lots of parameters (like a neural network) on a relatively simple task, and if your training dataset is relatively small, you are at risk of overfitting to the training dataset. Overfitting can lead to worse performance on data that wasn't used during training, such as test datasets or real new datapoints that your model will be applied to in production.\n",
    "\n",
    "One way to avoid overfitting is called \"early stopping\": split your training dataset into two pieces, which we'll call the \"training\" and \"validation\" splits. Then, train your model on the training split until the loss on the validation split stops going down. At this point, we have some evidence that the model is starting to memorize the training set, since its performance on the validation set is not improving. This method is not foolproof, but it's easy to use and gives one answer to the question \"When should I stop training?\" which you would have to answer anyway.\n",
    "\n",
    "Another way to avoid overfitting is called \"dropout\": during training, neurons are randomly turned off. This makes it harder for the model to memorize specific inputs. The neural network architecture (defined for you) below will make use of this.\n",
    "\n",
    "## Deep dream\n",
    "\n",
    "The goal of \"deep dream\" is to produce an image which produces strong activity in a unit in your neural network. This will help us understand what that unit is doing, since we can see what kinds of data it responds strongly to. We will perform a very simple version of the original deep dream experiment: find the input image which maximizes the activity of a neuron in a neural network trained to classify the MNIST digits. This will allow us to get some idea of what the network thinks a 0 is, or a 4 is, et cetera.\n",
    "\n",
    "\n",
    "[deepdream]: https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy5paMZrQRFQ"
   },
   "source": [
    "## Setup cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1698180349994,
     "user": {
      "displayName": "Kamiar Rahnama Rad",
      "userId": "17586984155005782915"
     },
     "user_tz": 240
    },
    "id": "5QTnT1G2lQmb",
    "outputId": "c6a2284a-8e22-4cf3-e34f-f151273ff4cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmIjcPhplWKn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load MNIST using torchvision\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "MNIST_train = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "MNIST_test = datasets.MNIST('../data', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1698180373962,
     "user": {
      "displayName": "Kamiar Rahnama Rad",
      "userId": "17586984155005782915"
     },
     "user_tz": 240
    },
    "id": "RPk2sVKCRIr1",
    "outputId": "1186dbcf-be3e-48cc-a5cd-9f77dc459665",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('training samples:', len(MNIST_train))\n",
    "print('testing samples:', len(MNIST_test))\n",
    "\n",
    "# Access a specific data point (e.g., the 10th data point)\n",
    "index = 10  # Change this to the index you want to access\n",
    "sample_image, label = MNIST_train[index]\n",
    "\n",
    "# Display the label and other information\n",
    "print(\"MNIST raw data\")\n",
    "print(f\"Data at index {index}:\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Image shape: {sample_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's show some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):  # Take example image 0,100,200,300,400\n",
    "    index = i*100\n",
    "    curr_f = MNIST_train[index][0]\n",
    "    \n",
    "    reshaped_curr_f = curr_f.permute(1, 2, 0)\n",
    "    \n",
    "    curr_lab = MNIST_train[index][1]\n",
    "    plt.title(\"Label: %d\" % curr_lab)\n",
    "    plt.imshow(reshaped_curr_f, cmap='gray')\n",
    "    plt.pause(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PqRr5cRQkIO"
   },
   "source": [
    "## Question 1: Training and validation split\n",
    "\n",
    "Using `numpy` and `pytorch`, randomly split the data in to a training and validation split. The training split should include 2/3 of the original data, and the validation split should include the remaining 1/3.\n",
    "\n",
    "(Hint): you can try creating random lists of indices that go into training and validation, then use [`torch.utils.data.Subset()`](https://pytorch.org/docs/stable/data.html) (imported as [`Subset`](https://pytorch.org/docs/stable/data.html)) to split the `torchvision` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8FDygsvQ7Z1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### QUESTION\n",
    "# Please fill in the following cells by splitting the datasets\n",
    "# `x_train_and_val` and `y_train_and_val`. You must assign name\n",
    "# results using the variable names below.\n",
    "# Create a train/validation split\n",
    "\n",
    "\n",
    "############# YOUR CODE HERE ###############\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzqH8BmNRid1"
   },
   "source": [
    "Here, we define a 3-layer multilayer perceptron with dropout and prepare for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7vJEMGVnFK-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a simple MLP network in PyTorch\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Set up loss and optimization\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvM1DYexSDUV"
   },
   "source": [
    "## Question 2: Early stopping\n",
    "\n",
    "Write a for loop which alternates between training the model on 1 pass through the training split (also known as 1 epoch of training) and checking whether we should stop early by measuring the validation loss and seeing if it is still decreasing.\n",
    "\n",
    "Please fill in the code to perform the validation step, including logic for early stopping. The code should have similar structure to how you might optimize on training data or evaluate on testing data.\n",
    "\n",
    "Please also print your validation loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63eo9axgS62g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### QUESTION\n",
    "max_n_epochs = 10\n",
    "\n",
    "\n",
    "prev_epoch_val_loss = np.inf\n",
    "for epoch in range(max_n_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set model to evaluation mode for validation\n",
    "    model.eval()\n",
    "    \n",
    "    ########## YOUR CODE HERE ###########\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWxGLl9NTkQe"
   },
   "source": [
    "As an extra check, we can look at the loss on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1663245465422,
     "user": {
      "displayName": "John Patrick Cunningham",
      "userId": "17427069175228633750"
     },
     "user_tz": 240
    },
    "id": "B6NBiWTxsrMe",
    "outputId": "555dd10c-c1b1-4374-bea7-47685129a542",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in train_dataloader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "\n",
    "test_loss /= len(dataset2)\n",
    "print(f\"Test: loss={test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWFDs-08Tv5y"
   },
   "source": [
    "### Question 3: Test data written answer question\n",
    "\n",
    "Would it have been good practice to use the test dataset instead of the validation split to perform early stopping above? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBcqswYlUAE8"
   },
   "source": [
    "**Your Answer Here**\n",
    "\n",
    "No, as we then cannot use the testing dataset for actual evaluation of the model. Our empirical risk estimate would become biased due to the use of the data in our training loop, even though we did not perform optimization steps with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2cfghvLQOq5"
   },
   "source": [
    "## Question 4: Basic \"deep dream\"\n",
    "\n",
    "(4.a) Implement the basic \"deep dream\"\n",
    "\n",
    "Our goal in this part of the problem is to find input images which maximally activate the output neuron corresponding to a particular class (for MNIST, that means corresponding to a particular digit). Let's pick the target class `0`.\n",
    "\n",
    "We'll do this the same way we trained our neural net: start from random images, and use stochastic gradient descent on some cost function to improve the images.\n",
    "\n",
    "Below, we have included code for randomly initializing the images, and for using the Adam optimizer to minimize a cost function. You are asked to fill in the cost function so that minimizing the cost function leads to maximizing the value of the \"0\" neuron in the output layer when the neural net is given `dream_images` as input. Make sure to use `dream_images` in your definition of `cost_function`.\n",
    "\n",
    "(Hint): What value of the model output might you want to optimize if you want an image that is most likely labeled `0`? Remember to sum over the batch of data as well, and that you will be minimizing the cost. Remember as well that our optimizer is taking the `dream images` as parameters, not the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHxhxk30Pt-b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### QUESTION\n",
    "# \"Deep dream\"\n",
    "# Construct random images\n",
    "\n",
    "n_dream_images = 16\n",
    "dream_images = torch.randn(n_dream_images, 1, 28, 28, requires_grad=True)\n",
    "optimizer = optim.Adam([dream_images], lr=0.01)\n",
    "\n",
    "########## YOUR CODE HERE ############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    cost = cost_function(dream_images)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "dream_images = dream_images.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHxhxk30Pt-b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the dream images\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=4, nrows=n_dream_images // 4, sharex=True, sharey=True, dpi=100\n",
    ")\n",
    "for im, ax in zip(dream_images, axes.flat):\n",
    "    ax.imshow(im[0].cpu().numpy(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "fig.suptitle(f\"Result at iteration {it}\")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Slg8FV0IcFI6"
   },
   "source": [
    "(4.b) After running the cell above to look at the result, do you have any reactions to what appears? In general, what can this basic version of deep dream tell us about our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6G6boWvcWvU"
   },
   "source": [
    "**Your Answer Here**\n",
    "\n",
    "(Open ended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
