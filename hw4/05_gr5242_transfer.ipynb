{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPIZFF6XOXbr"
      },
      "source": [
        "# **GR5242 HW04 Problem 5: Transfer learning with MobileNets**\n",
        "\n",
        "\n",
        "\n",
        "**For coding questions, look for filling in ########## Your code here ##########; for reflection questions, write down your answers in the \"Your Answer:\" text block.**\n",
        "\n",
        "There are many examples of external links to documentation. If you see reference to a `pytorch` or similar object, try hovering over the word to see if documentation is linked.\n",
        "\n",
        "## Description:\n",
        "In this homework, you will practice (1) going over the full procedure of training a neural network and (2) extending your knowledge on TF2, by implementing a transfer learning task. You will incorporate the existing [MobileNets](https://arxiv.org/abs/1704.04861) to your own network structure and to classify some new categories of images. Building and fitting the network from scratch is expensive and beyond the scope of this assignment, so you will load the MobileNets model which was pre-trained on the imagenet dataset. The version of MobileNet we're using is V2, which is a family of neural network architectures for efficient on-device image classification and related tasks.\n",
        "\n",
        "As a general advice, you can refer to the official documentations for more info if necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVVK2v2Agoao"
      },
      "source": [
        "**Import modules for later usage.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWTkgb1BMW5G",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"PyTorch version: \",torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUWUN8PyY0zz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDCoxCszzA4"
      },
      "source": [
        "## **Question 1: Download and format the data**\n",
        "\n",
        "The data we are going to use is the [Oxford flower dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/) which consists of 102 categories of flowers. Each class consists of between 40 and 258 images. The images can be found [here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/categories.html).\n",
        "\n",
        "The main difficulty of learninig from this dataset is in the large size of the classes. You may refer to [this paper](http://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/) for what other researchers have done with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVlqpl0-1IR8"
      },
      "source": [
        "### (1) Data Pre-processing\n",
        "\n",
        "First, load the  dataset from Kaggle (https://www.kaggle.com/competitions/oxford-102-flower-pytorch/data) where you can click \"Download All\" for the flower data. You can also download it directly from the zip file provided.\n",
        "\n",
        "Then you split the data into training and testing sets. How many training and testing samples do you have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaXys8Cx7nBq"
      },
      "source": [
        "During the pre-processing stage, we would like to format all the images for the MobileNet module.\n",
        "\n",
        "For this module, the size of the input image is fixed to height x width = 224 x 224 pixels. The input images are expected to have 3 RGB color values in the range [0, 1], following the common image input conventions (analogously to TF 1.x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTehUWLZMW5L",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize some parameters\n",
        "IMG_SIZE = 224\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    #do not restrain dim1 == dim2 == 224 here\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    #Custom transform to permute the dimensions\n",
        "    #data is (3, dim1, dim2), we want (dim1, dim2, 3)\n",
        "    #transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "])\n",
        "\n",
        "#Use validation set(contains labels) as a substitute for test,\n",
        "#as test set under Kaggle is hidden for submission to compete for highest accuracy\n",
        "\n",
        "\n",
        "# Assuming you have PyTorch datasets, replace root=' ' with your local directory.\n",
        "\n",
        "########## Your code here ##########\n",
        "from torchvision.datasets import Flowers102\n",
        "\n",
        "# Load the dataset\n",
        "#uses the built-in datase from torch, similar to that in tf\n",
        "raw_train = Flowers102(root='', split=\"train\", download=True)\n",
        "raw_test = Flowers102(root='', split=\"test\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ0KNOseY0z0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#-----------------------------------------------------------\n",
        "# this part is not necessary\n",
        "#for nn built-in flowers\n",
        "\n",
        "\n",
        "#(note that the size of the dataset does match the tf dataset)\n",
        "\n",
        "#raw_train\n",
        "print(len(raw_train))\n",
        "print(len(raw_test))\n",
        "\n",
        "# Access a specific data point (e.g., the 10th data point)\n",
        "index = 10  # Change this to the index you want to access\n",
        "sample_image, label = raw_train[index]\n",
        "\n",
        "# Display the label and other information\n",
        "print(\"nn raw data\")\n",
        "print(f\"Data at index {index}:\")\n",
        "print(f\"Label: {label}\")\n",
        "print(f\"Image shape: {sample_image.size}\")\n",
        "\n",
        "# Apply the raw data transforms to raw_train and raw_test\n",
        "train_nn = Flowers102(root='', split=\"train\", download=True, transform=transform)\n",
        "test_nn = Flowers102(root='', split=\"test\", download=True, transform=transform)\n",
        "\n",
        "#for nn build-in flowers: raw_train\n",
        "print(len(train_nn))\n",
        "print(len(test_nn))\n",
        "\n",
        "sample_image, label = train_nn[100]\n",
        "# Display the label and other information\n",
        "print(\"nn standardized image data\")\n",
        "print(f\"Data at index {index}:\")\n",
        "print(f\"Label: {label}\")\n",
        "print(f\"Image shape: {sample_image.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMMayPEha4u2"
      },
      "source": [
        "### (2) Data Exploration\n",
        "\n",
        "Let's plot some of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKPuuk75Y0z0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader, Dataset\n",
        "assert isinstance(train_nn, Dataset)\n",
        "assert isinstance(test_nn, Dataset)\n",
        "\n",
        "# Print the datasets\n",
        "print(train_nn)\n",
        "print(test_nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m10T7OGiY0z0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for i in range(5):  # Take example image 0,100,200,300,400\n",
        "    index = i*100\n",
        "    curr_f = train_nn[index][0]\n",
        "\n",
        "    #reshape each data form (3, 224, 224) to (224, 224, 3) to plot img correctly\n",
        "    reshaped_curr_f = curr_f.permute(1, 2, 0)\n",
        "\n",
        "    curr_lab = train_nn[index][1]\n",
        "    plt.title(\"Label: %d\" % curr_lab)\n",
        "    plt.imshow(reshaped_curr_f)\n",
        "    plt.pause(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAHm2JvGt6kn"
      },
      "source": [
        "### **Reflection Question (1a):**\n",
        "In the data exploration stage, what is the purpose of \"***assert isinstance(train, Dataset)***\"?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq0frUWXVmL5"
      },
      "source": [
        "**Your Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwYLLLUm86hP"
      },
      "source": [
        "## **Part 2: Self-defined CNN**\n",
        "\n",
        "In this section, you will define your own CNN (convolutional neural network) to classify the Oxford flowers.\n",
        "\n",
        "Recall from the first problem, to build a neural network using `torch`, we build a class that carries out the functions of the model, define an optimizer, and iterate through a few key steps.\n",
        "\n",
        "Here, we can make use of [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to save us a little hassle, now that we have seen how to build from the ground up in problem 1.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "One suggestion is that you build a model with the following architecture, although you are free to try others as well with the same idea:\n",
        "\n",
        "1.) Convolution with 3x3 kernel, input shape is the image shape. Make use of [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d), followed by [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU) and [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d) with `kernel_size` 2 and `stride` 2\n",
        "\n",
        "2.) Repeat step 1 (or a couple times), being careful about input shape\n",
        "\n",
        "3.) Convolution with 3x3 kernel, input shape is the image shape. Make use of [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d), followed by [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU) and [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten)\n",
        "\n",
        "4.) Fully connected layer using [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear) and [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU)\n",
        "\n",
        "5.)[`torch.nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout)\n",
        "\n",
        "6.) Linear layer returning us to number of classes (102)\n",
        "\n",
        "7.) [`nothing`] or [`torch.nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax) to get label likelihood. Remember now that depending on which of these you use, you will need either [`criterion = nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss) or [`criterion = nn.NLLLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss) in training. If you use `nn.CrossEntropyLoss()`, you will need the extra step of calling [`nn.functional.softmax(output, dim=1)`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax) to compare outputs to targets in model evaluation, but not before calculating the loss in your training loop.\n",
        "\n",
        "After fitting the model, please test the accuracy of the prediction on the test set.\n",
        "\n",
        "In this stage, we do not ask for a great performance (you should have 'some' predictive performance though). But please ensure that you obtain a trainable model with no programming bugs. You may find it helpful to print the training progress bar or epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhVKhSKSY0z1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 1: Model definition\n",
        "# Use a nn.Sequential model for deining your own CNN\n",
        "\n",
        "########## Your code here ##########\n",
        "# Define the model using nn.Sequential, naming it model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDgcqQREwyl1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Optional: print a summary of your model\n",
        "# from torchsummary import summary\n",
        "\n",
        "# Assuming your model_transfer is defined, you can print the summary\n",
        "# summary(our_model, (3, 224, 224))  # Assuming input size is (3, 224, 224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ZAUIYfY0z1"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "Here we will prepare ourselves for training.\n",
        "\n",
        "We need to define a few things before running our training loop, namely the `DataLoader`, `criterion`, [`optimizer`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html), and [`lr_scheduler`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y82V2Je4Y0z1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 2: Model compilation\n",
        "# Be sure to specify the optimizer, loss and metric as required\n",
        "\n",
        "\n",
        "# Assuming you have a PyTorch model named 'model'\n",
        "train_loader_nn = DataLoader(train_nn, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "# Define your loss function, naming it 'criterion'\n",
        "########## Your code here ##########\n",
        "\n",
        "\n",
        "# Define your optimizer, 'optimizer' (Adam is suggested)\n",
        "########## Your code here ##########\n",
        "\n",
        "\n",
        "# Learning rate scheduler, 'lr_scheduler' (StepLR is suggested)\n",
        "########## Your code here ##########\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2gh_P8FY0z2"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "Fill in necessary blanks in the training loop, with the provided guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeWcEFZGY0z2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 3: Model fitting\n",
        "# Use the prepared training data to fit your CN\n",
        "# Training loop\n",
        "epochs = 15  # Set the number of training epochs\n",
        "# Total number of data points in the dataset (number of points per epoch)\n",
        "n_train = len(train_loader_nn.dataset)\n",
        "\n",
        "print(\"start to train\")\n",
        "for epoch in range(epochs):\n",
        "    #print(\"epoch \" + str(epoch))\n",
        "    model.train()\n",
        "    total_accuracy = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader_nn:\n",
        "        inputs, labels = batch\n",
        "        # zero the gradient, calculate outputs, calculate loss,\n",
        "        # backpropagate, and take a step\n",
        "        # this should be familiar from earlier problems in the assignment\n",
        "        ########## YOUR CODE HERE ###########\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate accuracy for this batch\n",
        "        predicted_classes = outputs.argmax(dim=1)\n",
        "        batch_accuracy = (predicted_classes == labels).float().sum()\n",
        "        total_accuracy += batch_accuracy.item()\n",
        "        total_loss += loss.item() * len(labels)\n",
        "\n",
        "\n",
        "    # Calculate the average accuracy for the entire epoch\n",
        "    average_accuracy = total_accuracy / n_train\n",
        "    average_loss = total_loss / n_train\n",
        "    # Take a step in learning rate with 'lr_scheduler`\n",
        "    ############# YOUR CODE HERE ############### Adjust learning rate\n",
        "\n",
        "    print(f\" Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.3f}, Accuracy: {average_accuracy * 100:.2f}%\")\n",
        "    #print(\"len train_loader is \" + str(len(train_loader_nn)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qimLBQvZY0z2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 4: Model evaluation\n",
        "test_dataloader_nn = DataLoader(test_nn, batch_size=32, shuffle=True)\n",
        "n_test = len(test_dataloader_nn.dataset)  # Total number of test data points\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "    test_total_accuracy = 0.0\n",
        "    test_total_loss = 0.0\n",
        "\n",
        "    # Iterate through the test dataset\n",
        "    for test_batch in test_dataloader_nn:\n",
        "        test_inputs, test_labels = test_batch\n",
        "\n",
        "        # Forward pass\n",
        "        test_outputs = model(test_inputs)\n",
        "\n",
        "        # Calculate the loss for the current batch\n",
        "        test_loss = criterion(test_outputs, test_labels)\n",
        "\n",
        "        # Accumulate the total loss (convert average loss to sum)\n",
        "        test_total_loss += test_loss.item() * len(test_labels)\n",
        "\n",
        "        # Calculate the accuracy for the current batch\n",
        "        test_predicted_classes = test_outputs.argmax(dim=1)\n",
        "        test_batch_accuracy = (test_predicted_classes == test_labels).float().sum()\n",
        "        test_total_accuracy += test_batch_accuracy\n",
        "\n",
        "    # Compute the average accuracy over the entire test set\n",
        "    test_average_accuracy = test_total_accuracy / n_test\n",
        "\n",
        "    # Compute the average loss over the entire test set\n",
        "    test_average_loss = test_total_loss / n_test\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Test Loss: {test_average_loss:.4f}   Test Accuracy: {test_average_accuracy.item() * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJhekDqT1tIA"
      },
      "source": [
        "### **Reflection Questions 2a:**\n",
        "\n",
        "(1) How did you choose your network structure? \\\\\n",
        "(2) Which optimizer did you use? Why? \\\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOW2R90BGNvQ"
      },
      "source": [
        "**Your Answer:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzw_edTC1FKY"
      },
      "source": [
        "## **Part 3: Transfer Learning Using Pre-trained Model**\n",
        "\n",
        "There are several types of transfer learning, as illustrated [here](http://ronny.rest/blog/post_2017_10_13_tf_transfer_learning/). In this homework, you will practice B2, using MobileNet_V2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NITos9iAK7Q"
      },
      "source": [
        "### (1) Freeze the pre-trained model and fine-tune the transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtD1xWxdY0z2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model\n",
        "MobileNetV2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "MobileNetV2.classifier = nn.Identity()  # Remove the classifier layers\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "MobileNetV2.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2okjiZnKyucO"
      },
      "source": [
        "Now you can go through the same steps to build and train the transfer learning model.\n",
        "\n",
        "### Instructions:\n",
        "Within the `model_transfer = nn.Sequential()` call, dd an Adaptive Average Pooling layer with [`nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d), then perform flattening and apply a linear layer as you should be familiar with from earlier.\n",
        "\n",
        "As before, remember your choice of whether to use Cross Entropy or Negative Log Likelihood, and make sure to use the corresponding output of your model (i.e., whether to apply Softmax after calculating loss or within the model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU8ai2whY0z3"
      },
      "outputs": [],
      "source": [
        "# Step 1: Model definition\n",
        "# Use a torch.nn Sequential model for defining the transfer learning model (B1 style)\n",
        "\n",
        "# Set MobileNetV2 parameters to non-trainable\n",
        "for param in MobileNetV2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Use a custom reshape layer\n",
        "class ReshapeLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), x.size(1), 1, 1)\n",
        "\n",
        "# Create a Sequential model in PyTorch\n",
        "model_transfer = nn.Sequential(\n",
        "    MobileNetV2,\n",
        "    ReshapeLayer(),  # Reshape to [batch_size, num_channels, 1, 1]\n",
        "\n",
        "########## Your code here ##########\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSERSDyDq1mt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# define batch size here\n",
        "batch_size = 32\n",
        "input_tensor = torch.randn([batch_size, 3, 224, 224])\n",
        "\n",
        "# visualize the model graphical structure\n",
        "#Iterate through the model and print the dimensions at each layer\n",
        "for layer in model_transfer:\n",
        "    input_tensor = layer(input_tensor)\n",
        "    print(f\"Layer: {layer.__class__.__name__}, Output Shape: {input_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6uL2mWLY0z3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#print(model_transfer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKugnpb3Y0z3"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "As before, write code to define your `optimizer`, `DataLoader`, `loss (criterion)`, and `lr_scheduler`.\n",
        "\n",
        "Then, write a training loop.\n",
        "\n",
        "Your code here should look similar to earlier in the assignment, outside of choosing hyperparameters, names, and possibly choice of loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UifjkmabY0z3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 2: Prepare for training\n",
        "# Be sure to specify the optimizer, loss, DataLoader, lr_scheduler as before\n",
        "\n",
        "########## Your code here ##########\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "\n",
        "# Step 3: Model fitting\n",
        "# Use the prepared training data to fit the transfer learning model\n",
        "\n",
        "########## Your code here ##########\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apsr2O5pY0z3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 3: Model evaluation\n",
        "# Use the prepared testing data to evaluate the transfer learning model\n",
        "\n",
        "#test_dataloader_nn = DataLoader(test_nn, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"model_transfer\")\n",
        "model_transfer.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "\n",
        "    test_total_accuracy = 0.0\n",
        "    test_total_loss = 0.0  # Reset total loss at the beginning of each epoch\n",
        "\n",
        "    for test_batch in test_dataloader_nn:\n",
        "        test_inputs, test_labels = test_batch\n",
        "        test_outputs = model_transfer(test_inputs)\n",
        "        test_loss = criterion(test_outputs, test_labels)\n",
        "        # Accumulate the total loss (convert average loss to sum)\n",
        "        test_total_loss += test_loss.item() * len(test_labels)\n",
        "        _, test_predicted = torch.max(test_outputs, 1)  # Get the predicted class\n",
        "        test_batch_accuracy = (test_predicted == test_labels).float().sum()\n",
        "        test_total_accuracy += test_batch_accuracy.item()\n",
        "\n",
        "\n",
        "    # Compute the average accuracy over the entire test set\n",
        "    test_average_accuracy = test_total_accuracy / n_test\n",
        "\n",
        "    # Compute the average loss over the entire test set\n",
        "    test_average_loss = test_total_loss / n_test\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Test Loss: {test_average_loss:.3f}   Test Accuracy: {test_average_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-7FrsTq0lr-"
      },
      "source": [
        "### (2) Fine-tune some parameters in your network to see if you can improve the performance on testing data. (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWB5FO9p1IeN"
      },
      "outputs": [],
      "source": [
        "########## Your code here ##########\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceNCDgEKhsYW"
      },
      "source": [
        "### **Reflection Questions 3a:**\n",
        "\n",
        "(1) Briefly explain the network structure of MobileNet and how is it different from other models? \\\n",
        "(2) In your experiment, which parameter(s) is the network most sensitive to? Can you briefly reason why? \\\n",
        "(3) What are some pros and cons of doing transfer learning? \\\n",
        "(4) What is a batch? How does the batch size affect the training process? \\\n",
        "(5) What is an epoch during the training process?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS_zHWlHtkXF"
      },
      "source": [
        "**Your Answer:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kUvmlC_Y0z4"
      },
      "source": [
        "(6) Describe any observation you find interesting from the above experiment (Open-ended)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6AuPCbtY0z8"
      },
      "source": [
        "**Your Answer:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJTpk0YY0z8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
